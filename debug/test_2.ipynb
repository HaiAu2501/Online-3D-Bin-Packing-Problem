{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from models.transformer import BinPackingTransformer\n",
    "from env.env import BinPacking3DEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformer model\n",
    "transformer = BinPackingTransformer(\n",
    "\td_model=128,\n",
    "\tn_head=8,\n",
    "\tn_layers=3,\n",
    "\td_feedforward=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinPackingTransformer(\n",
       "  (ems_list_embedding): Embedding(\n",
       "    (linear): Linear(in_features=6, out_features=128, bias=True)\n",
       "  )\n",
       "  (buffer_embedding): Embedding(\n",
       "    (linear): Linear(in_features=3, out_features=128, bias=True)\n",
       "  )\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-2): 3 x TransformerBlock(\n",
       "      (self_attn_ems_list): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (self_attn_buffer): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1_ems_list): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm1_buffer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_ems_list): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (mlp_buffer): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm2_ems_list): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2_buffer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (cross_attn_ems_list): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn_buffer): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm3_ems_list): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3_buffer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_final_ems_list): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (mlp_final_buffer): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm4_ems_list): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm4_buffer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (combined_pooling): CombinedPooling(\n",
       "    (attn): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment and dummy input to test the model\n",
    "env = BinPacking3DEnv(\n",
    "\tbin_size=(5, 5, 5),\n",
    "\titems=[(2, 3, 1), (2, 2, 3), (1, 1, 2), (3, 2, 2)],\n",
    "\tbuffer_size=2,\n",
    "\tnum_rotations=2,\n",
    "\tmax_ems=100,\n",
    ")\n",
    "\n",
    "obervation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EMS list\n",
    "ems_list = obervation['ems_list']\n",
    "\n",
    "ems_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ems_mask = obervation['ems_mask']\n",
    "\n",
    "ems_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Buffer\n",
    "buffer = obervation['buffer']\n",
    "\n",
    "buffer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 100, 6), (1, 2, 3))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ems_list_np = np.expand_dims(ems_list, axis=0)  # [1, max_ems, 6]\n",
    "buffer_np = np.expand_dims(buffer, axis=0)      # [1, buffer_size, 3]\n",
    "\n",
    "ems_list_np.shape, buffer_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100, 6]), torch.Size([1, 2, 3]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ems_list_tensor = torch.tensor(ems_list_np, dtype=torch.float32)\n",
    "buffer_tensor = torch.tensor(buffer_np, dtype=torch.float32)\n",
    "\n",
    "ems_list_tensor.shape, buffer_tensor.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
